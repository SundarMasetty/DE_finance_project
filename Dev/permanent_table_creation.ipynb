{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae7e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.3.0\n",
      "Hive Catalog Implementation: in-memory\n",
      "Warehouse Directory: file:/home/jupyter/spark-warehouse\n",
      "HiveContext is available\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark Version:\", spark.version)\n",
    "print(\"Hive Catalog Implementation:\", spark.conf.get(\"spark.sql.catalogImplementation\"))\n",
    "print(\"Warehouse Directory:\", spark.conf.get(\"spark.sql.warehouse.dir\"))\n",
    "\n",
    "# Check if Hive classes are available\n",
    "try:\n",
    "    from pyspark.sql import HiveContext\n",
    "    print(\"HiveContext is available\")\n",
    "except ImportError:\n",
    "    print(\"HiveContext not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7807b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive Catalog Implementation: hive\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create new SparkSession with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"External_table_for_analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/home/jupyter/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Hive support is now enabled\n",
    "print(\"Hive Catalog Implementation:\", spark.conf.get(\"spark.sql.catalogImplementation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4502832",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data_df = spark.read.format(\"parquet\").load(\"Lending_club_project/cleaned/customers_data_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61825309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[member_id: string, emp_title: string, emp_length: int, home_ownership: string, annual_income: float, address_state: string, address_zipcode: string, address_country: string, grade: string, sub_grade: string, verification_status: string, total_high_credit_limit: float, application_type: string, join_annual_income: float, verification_status_joint: string, ingest_date: timestamp]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_data_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5575b406",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Namespace 'lendingclubdata' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_681/2303645440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE DATABASE lendingclubdata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Namespace 'lendingclubdata' already exists"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE lendingclubdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe974ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view 'customers' already exists in database 'lendingclubdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_681/1722582235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mSTORED\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mPARQUET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mLOCATION\u001b[0m \u001b[0;34m'Lending_club_project/cleaned/customers_data_parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \"\"\")\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view 'customers' already exists in database 'lendingclubdata'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create in default database\n",
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE lendingclubdata.customers(\n",
    "    member_id string, \n",
    "    emp_title string, \n",
    "    emp_length int, \n",
    "    home_ownership string, \n",
    "    annual_income float, \n",
    "    address_state string, \n",
    "    address_zipcode string, \n",
    "    address_country string,\n",
    "    grade string, \n",
    "    sub_grade string, \n",
    "    verification_status string, \n",
    "    total_high_credit_limit float, \n",
    "    application_type string,\n",
    "    join_annual_income float, \n",
    "    verification_status_joint string, \n",
    "    ingest_date timestamp\n",
    ") \n",
    "STORED AS PARQUET \n",
    "LOCATION 'Lending_club_project/cleaned/customers_data_parquet'\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22771c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I need to create EXTERNAL TABLE as on above code but as i am working on local and having issues with hive metastore creating table which i can access across my cluster\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lendingclubdata\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS lendingclubdata.customers\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE lendingclubdata.customers (\n",
    "    member_id string, \n",
    "    emp_title string, \n",
    "    emp_length int, \n",
    "    home_ownership string, \n",
    "    annual_income float, \n",
    "    address_state string, \n",
    "    address_zipcode string, \n",
    "    address_country string,\n",
    "    grade string, \n",
    "    sub_grade string, \n",
    "    verification_status string, \n",
    "    total_high_credit_limit float, \n",
    "    application_type string,\n",
    "    join_annual_income float, \n",
    "    verification_status_joint string, \n",
    "    ingest_date timestamp\n",
    ") USING PARQUET\n",
    "LOCATION 'file:////home/jupyter/Lending_club_project/cleaned/customers_data_parquet'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"REFRESH TABLE lendingclubdata.customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c38644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
      "|           member_id|           emp_title|emp_length|home_ownership|annual_income|address_state|address_zipcode|address_country|grade|sub_grade|verification_status|total_high_credit_limit|application_type|join_annual_income|verification_status_joint|         ingest_date|\n",
      "+--------------------+--------------------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
      "|0d3c568ff6944b11c...|Bookkeeper/Accoun...|        10|      MORTGAGE|      48000.0|           SC|          297xx|            USA|    C|       C5|       Not Verified|               298100.0|      Individual|              null|                     null|2025-08-25 16:11:...|\n",
      "|170564954b4d02ae9...|  Billing Supervisor|        10|      MORTGAGE|      80000.0|           TX|          774xx|            USA|    C|       C5|       Not Verified|               197904.0|      Individual|              null|                     null|2025-08-25 16:11:...|\n",
      "+--------------------+--------------------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lendingclubdata.customers\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "093a7a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------+-------+\n",
      "|col_name                    |data_type      |comment|\n",
      "+----------------------------+---------------+-------+\n",
      "|member_id                   |string         |null   |\n",
      "|emp_title                   |string         |null   |\n",
      "|emp_length                  |int            |null   |\n",
      "|home_ownership              |string         |null   |\n",
      "|annual_income               |float          |null   |\n",
      "|address_state               |string         |null   |\n",
      "|address_zipcode             |string         |null   |\n",
      "|address_country             |string         |null   |\n",
      "|grade                       |string         |null   |\n",
      "|sub_grade                   |string         |null   |\n",
      "|verification_status         |string         |null   |\n",
      "|total_high_credit_limit     |float          |null   |\n",
      "|application_type            |string         |null   |\n",
      "|join_annual_income          |float          |null   |\n",
      "|verification_status_joint   |string         |null   |\n",
      "|ingest_date                 |timestamp      |null   |\n",
      "|                            |               |       |\n",
      "|# Detailed Table Information|               |       |\n",
      "|Database                    |lendingclubdata|       |\n",
      "|Table                       |customers      |       |\n",
      "+----------------------------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE formatted lendingclubdata.customers\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38576a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loans data \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lendingclubdata\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS lendingclubdata.loans\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE lendingclubdata.loans(loan_id string, member_id string, loan_amnt float, funded_amnt float, loan_term_months string,\n",
    "           interest_rate float, monthly_installment float, issue_date string, loan_status string, \n",
    "          loan_purpose string, loan_title string, ingested_time timestamp)\n",
    "          USING PARQUET LOCATION 'file:////home/jupyter/Lending_club_project/cleaned/loan_data_parquet'\n",
    "          \"\"\")\n",
    "spark.sql(\"REFRESH TABLE lendingclubdata.loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b73ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+-----------+----------------+-------------+-------------------+----------+-----------+------------------+------------------+--------------------+\n",
      "|  loan_id|           member_id|loan_amnt|funded_amnt|loan_term_months|interest_rate|monthly_installment|issue_date|loan_status|      loan_purpose|        loan_title|       ingested_time|\n",
      "+---------+--------------------+---------+-----------+----------------+-------------+-------------------+----------+-----------+------------------+------------------+--------------------+\n",
      "|130943487|51989e17caab932ca...|   4800.0|     4800.0|            null|        15.04|             166.49|  Mar-2018| Fully Paid|           medical|  Medical expenses|2025-08-26 00:17:...|\n",
      "|130514193|043da8398b63b2c44...|  40000.0|    40000.0|            null|        14.07|             932.19|  Mar-2018|    Current|debt_consolidation|Debt consolidation|2025-08-26 00:17:...|\n",
      "+---------+--------------------+---------+-----------+----------------+-------------+-------------------+----------+-----------+------------------+------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM lendingclubdata.loans\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8db27bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------+-----------------------+-----------------------+----------------------+-------------------+-----------------+-----------------+--------------------+\n",
      "|  loan_id|total_principal_received|total_interest_received|total_late_fee_received|total_payment_received|last_payment_amount|last_payment_date|next_payment_date|         ingest_date|\n",
      "+---------+------------------------+-----------------------+-----------------------+----------------------+-------------------+-----------------+-----------------+--------------------+\n",
      "|113619662|                  7500.0|                 357.79|                    0.0|              7857.791|            6642.78|         Feb-2018|             null|2025-08-26 11:43:...|\n",
      "|111985436|                15937.81|                3123.29|                    0.0|               19061.1|            1002.33|         Feb-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113569389|                10106.07|                3470.43|                    0.0|               13576.5|             679.58|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113855914|                  6000.0|                 894.66|                    0.0|              6894.661|            3840.99|         Nov-2018|             null|2025-08-26 11:43:...|\n",
      "|113839616|                 2799.97|                2226.58|                    0.0|               5026.55|             251.75|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113529525|                 2244.02|                 831.44|                    0.0|               3075.46|             147.71|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113851343|                 4400.78|                1155.75|                    0.0|               5556.53|             292.19|         Feb-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|112729968|                  4230.9|                  721.7|                    0.0|                4952.6|             247.79|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|114073825|                 5525.27|                5220.64|                    0.0|              10745.91|             538.28|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|114171262|                 8329.71|                1906.54|                    0.0|              10236.25|             512.08|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113684273|                 2526.63|                 867.59|                    0.0|               3394.22|              169.9|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|114023823|                 2143.41|                 264.61|                    0.0|               2408.02|             120.46|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113307810|                 25000.0|                1280.65|                    0.0|             26280.654|           14692.13|         Feb-2018|             null|2025-08-26 11:43:...|\n",
      "|113876028|                 16000.0|                1278.58|                    0.0|             17278.582|           15735.78|         Dec-2017|             null|2025-08-26 11:43:...|\n",
      "|113881466|                  1600.0|                 131.63|                    0.0|             1731.6311|             939.31|         Dec-2018|             null|2025-08-26 11:43:...|\n",
      "|113206202|                 5741.09|                 1478.7|                    0.0|               7219.79|             335.12|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113541874|                10230.05|                2833.32|                    0.0|              13063.37|             686.62|         Feb-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113608833|                 5112.03|                1523.31|                    0.0|               6635.34|              332.1|         Mar-2019|         Apr-2019|2025-08-26 11:43:...|\n",
      "|113618697|                  9000.0|                1671.37|                    0.0|             10671.366|            5371.32|         Jan-2019|             null|2025-08-26 11:43:...|\n",
      "|113811106|                 20000.0|                1561.37|                    0.0|              21561.37|           13426.25|         Sep-2018|             null|2025-08-26 11:43:...|\n",
      "+---------+------------------------+-----------------------+-----------------------+----------------------+-------------------+-----------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loan repayments data\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lendingclubdata\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS lendingclubdata.loan_repayments\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE lendingclubdata.loan_repayments(loan_id string,total_principal_received float,total_interest_received float,\n",
    "          total_late_fee_received float,total_payment_received float,last_payment_amount float,last_payment_date string,\n",
    "          next_payment_date string,ingest_date timestamp) \n",
    "          USING PARQUET LOCATION 'file:////home/jupyter/Lending_club_project/cleaned/loan_repayments_parquet' \"\"\")\n",
    "\n",
    "spark.sql(\"REFRESH TABLE lendingclubdata.loan_repayments\")\n",
    "spark.sql(\"SELECT * FROM lendingclubdata.loan_repayments\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c191f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+----------------------+\n",
      "|           member_id|delinq_2yrs|delinq_amnt|mths_since_last_delinq|\n",
      "+--------------------+-----------+-----------+----------------------+\n",
      "|680fb82b4acb97795...|          0|        0.0|                    24|\n",
      "|0e0a6f18a90718c2a...|          0|        0.0|                    54|\n",
      "+--------------------+-----------+-----------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#loan delinquers\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lendingclubdata\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS lendingclubdata.delinq\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE lendingclubdata.delinq(member_id string,delinq_2yrs integer,delinq_amnt float,mths_since_last_delinq integer )\n",
    "          USING PARQUET LOCATION 'file:////home/jupyter/Lending_club_project/cleaned/loans_defaulters_delinq_parquet' \"\"\")\n",
    "spark.sql(\"REFRESH TABLE lendingclubdata.delinq\")\n",
    "spark.sql(\"SELECT * FROM lendingclubdata.delinq\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84083fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------+\n",
      "|           member_id|pub_rec|pub_rec_bankruptcies|inq_last_6mths|\n",
      "+--------------------+-------+--------------------+--------------+\n",
      "|5c18f413cebed2192...|      1|                   1|             0|\n",
      "|e98eb408f9863ef59...|      0|                   0|             0|\n",
      "+--------------------+-------+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS lendingclubdata\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "DROP TABLE IF EXISTS lendingclubdata.loan_defaulters_detail_rec\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE lendingclubdata.loan_defaulters_detail_rec(member_id string, pub_rec int, pub_rec_bankruptcies int, inq_last_6mths int) USING PARQUET LOCATION 'file:////home/jupyter/Lending_club_project/cleaned/loans_defaulters_detailed_records_parquet'\"\"\")\n",
    "spark.sql(\"REFRESH TABLE lendingclubdata.loan_defaulters_detail_rec\")\n",
    "spark.sql(\"SELECT * FROM lendingclubdata.loan_defaulters_detail_rec\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ad3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+-----------+\n",
      "|namespace      |tableName                 |isTemporary|\n",
      "+---------------+--------------------------+-----------+\n",
      "|lendingclubdata|customers                 |false      |\n",
      "|lendingclubdata|delinq                    |false      |\n",
      "|lendingclubdata|loan_defaulters_detail_rec|false      |\n",
      "|lendingclubdata|loan_repayments           |false      |\n",
      "|lendingclubdata|loans                     |false      |\n",
      "+---------------+--------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES FROM lendingclubdata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abf87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      namespace|\n",
      "+---------------+\n",
      "|        default|\n",
      "|lendingclubdata|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff344e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating view  customer_complete_view\n",
    "spark.sql(\"USE DATABASE lendingclubdata\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW customer_complete_view AS\n",
    "SELECT \n",
    "    c.member_id,\n",
    "    c.emp_title,\n",
    "    c.emp_length,\n",
    "    c.home_ownership,\n",
    "    c.annual_income,\n",
    "    c.address_state,\n",
    "    c.address_zipcode,\n",
    "    c.address_country,\n",
    "    c.grade,\n",
    "    c.sub_grade,\n",
    "    c.verification_status,\n",
    "    c.total_high_credit_limit,\n",
    "    c.application_type,\n",
    "    c.join_annual_income,\n",
    "    c.verification_status_joint,\n",
    "    \n",
    "    -- Loans data\n",
    "    l.loan_id,\n",
    "    l.loan_amnt,\n",
    "    l.funded_amnt,\n",
    "    l.loan_term_months,\n",
    "    l.interest_rate,\n",
    "    l.monthly_installment,\n",
    "    l.issue_date,\n",
    "    l.loan_status,\n",
    "    l.loan_purpose,\n",
    "    l.loan_title,\n",
    "    \n",
    "    -- Loan repayments data\n",
    "    lr.total_principal_received,\n",
    "    lr.total_interest_received,\n",
    "    lr.total_late_fee_received,\n",
    "    lr.total_payment_received,\n",
    "    lr.last_payment_amount,\n",
    "    lr.last_payment_date,\n",
    "    lr.next_payment_date,\n",
    "    \n",
    "    -- Delinquency data\n",
    "    d.delinq_2yrs,\n",
    "    d.delinq_amnt,\n",
    "    d.mths_since_last_delinq,\n",
    "    \n",
    "    -- Loan defaulters detail data\n",
    "    ldd.pub_rec,\n",
    "    ldd.pub_rec_bankruptcies,\n",
    "    ldd.inq_last_6mths\n",
    "\n",
    "FROM customers c\n",
    "LEFT JOIN loans l ON c.member_id = l.member_id\n",
    "LEFT JOIN loan_repayments lr ON l.loan_id = lr.loan_id\n",
    "LEFT JOIN delinq d ON c.member_id = d.member_id\n",
    "LEFT JOIN loan_defaulters_detail_rec ldd ON c.member_id = ldd.member_id;\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9afc75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+-----------+\n",
      "|namespace      |viewName              |isTemporary|\n",
      "+---------------+----------------------+-----------+\n",
      "|lendingclubdata|customer_complete_view|false      |\n",
      "+---------------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW VIEWS\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b74051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------+---------+-----------+----------------+-------------+-------------------+----------+-----------+------------------+--------------------+------------------------+-----------------------+-----------------------+----------------------+-------------------+-----------------+-----------------+-----------+-----------+----------------------+-------+--------------------+--------------+\n",
      "|           member_id|           emp_title|emp_length|home_ownership|annual_income|address_state|address_zipcode|address_country|grade|sub_grade|verification_status|total_high_credit_limit|application_type|join_annual_income|verification_status_joint| loan_id|loan_amnt|funded_amnt|loan_term_months|interest_rate|monthly_installment|issue_date|loan_status|      loan_purpose|          loan_title|total_principal_received|total_interest_received|total_late_fee_received|total_payment_received|last_payment_amount|last_payment_date|next_payment_date|delinq_2yrs|delinq_amnt|mths_since_last_delinq|pub_rec|pub_rec_bankruptcies|inq_last_6mths|\n",
      "+--------------------+--------------------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------+---------+-----------+----------------+-------------+-------------------+----------+-----------+------------------+--------------------+------------------------+-----------------------+-----------------------+----------------------+-------------------+-----------------+-----------------+-----------+-----------+----------------------+-------+--------------------+--------------+\n",
      "|2654fef9a999e35b8...|Sr Director Engin...|        10|      MORTGAGE|     250000.0|           WA|          982xx|            USA|    A|       A5|    Source Verified|               631203.0|      Individual|              null|                     null|68536929|   6000.0|     6000.0|            null|         7.91|             187.77|  Dec-2015| Fully Paid|             other|               Other|                  6000.0|                 114.01|                    0.0|               6114.01|            5745.06|         Apr-2016|             null|          1|        0.0|                    19|      0|                   0|             1|\n",
      "|49c691121315ceeac...|            mechanic|         5|          RENT|      35000.0|           KS|          672xx|            USA|    C|       C5|    Source Verified|                21822.0|      Individual|              null|                     null|67849662|   4225.0|     4225.0|            null|        14.85|             146.16|  Dec-2015|Charged Off|debt_consolidation|  Debt consolidation|                  1536.9|                  653.6|                    0.0|               2558.87|             146.16|         Apr-2017|             null|          2|        0.0|                    18|      0|                   0|             0|\n",
      "|4d2b9e5c6ee90df7e...|                  GM|         6|          RENT|     110000.0|           IL|          606xx|            USA|    C|       C5|       Not Verified|                45509.0|      Individual|              null|                     null|68426865|  20000.0|    20000.0|            null|        14.85|             691.84|  Dec-2015| Fully Paid|       credit_card|Credit card refin...|                 20000.0|                4889.01|                    0.0|             24889.014|             1407.7|         Dec-2018|             null|       null|       null|                  null|      0|                   0|             0|\n",
      "+--------------------+--------------------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------+---------+-----------+----------------+-------------+-------------------+----------+-----------+------------------+--------------------+------------------------+-----------------------+-----------------------+----------------------+-------------------+-----------------+-----------------+-----------+-----------+----------------------+-------+--------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer_complete_view\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "906e48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+---------+\n",
      "|member_id                                                       |total_cnt|\n",
      "+----------------------------------------------------------------+---------+\n",
      "|ca5fd93b4f9adf94118962b8d3e3d24e4810d891d1dbcaa0f98e641e16d4b4a2|3        |\n",
      "|27bdc71bcc167a89e07e981246320dbdaddc8c67354ed022f3e835368da0fb0d|3        |\n",
      "|ab24d776473f88620dd571bb74ad88889daee02904f82aa07ae91a4c69e6ec58|3        |\n",
      "|460e1a4a960113bbfd4d3defca860fc4e7387f7b40270d5f78cb469a3baceda1|2        |\n",
      "|912a3ce5af18a80b2ca3453db9a63d16baeecf27ae01f97bdb062770915ec8ac|2        |\n",
      "+----------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identifying the bad data\n",
    "spark.sql(\"\"\"\n",
    "SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.customers GROUP BY member_id  ORDER BY total_cnt DESC;\n",
    "          \"\"\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "534be475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
      "|           member_id|emp_title|emp_length|home_ownership|annual_income|address_state|address_zipcode|address_country|grade|sub_grade|verification_status|total_high_credit_limit|application_type|join_annual_income|verification_status_joint|         ingest_date|\n",
      "+--------------------+---------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
      "|ca5fd93b4f9adf941...|     null|         6|          RENT|      30000.0|           NY|          104xx|            USA|    C|       C5|           Verified|                31620.0|      Individual|              null|                     null|2025-08-25 16:11:...|\n",
      "|ca5fd93b4f9adf941...|     null|         6|          RENT|      30000.0|           NY|          104xx|            USA|    C|       C5|           Verified|                22304.0|      Individual|              null|                     null|2025-08-25 16:11:...|\n",
      "|ca5fd93b4f9adf941...|     null|         6|          RENT|      30000.0|           NY|          104xx|            USA|    C|       C5|           Verified|                55000.0|      Individual|              null|                     null|2025-08-25 16:11:...|\n",
      "+--------------------+---------+----------+--------------+-------------+-------------+---------------+---------------+-----+---------+-------------------+-----------------------+----------------+------------------+-------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT * FROM lendingclubdata.customers\n",
    "           WHERE member_id = 'ca5fd93b4f9adf94118962b8d3e3d24e4810d891d1dbcaa0f98e641e16d4b4a2' ;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "182f6d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+---------+\n",
      "|member_id                                                       |total_cnt|\n",
      "+----------------------------------------------------------------+---------+\n",
      "|74a3e0f8c9e64b9ddb788e83b01a93e74b4f1a7f0653b17bbfa594b3e4e3aa29|2        |\n",
      "|96bdf6e6b517986899048173d01be098b7b0600ad4bbabfc14a58198da2ae537|2        |\n",
      "|a380e78c166ab255ce6c71c6ba6b2072a868c057d7681694e6fd8fab7b7158d5|2        |\n",
      "|7a7514d802a29e28ac347989eb0341a74138f34d005cbf3187b8f37070f720bd|2        |\n",
      "|678da331e66060c87b970cc880604cf86656ca341b6d22ffe629d141caf2c775|2        |\n",
      "+----------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.delinq GROUP BY member_id  ORDER BY total_cnt DESC;\n",
    "          \"\"\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b87cdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+---------+\n",
      "|member_id                                                       |total_cnt|\n",
      "+----------------------------------------------------------------+---------+\n",
      "|e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855|6        |\n",
      "|ab24d776473f88620dd571bb74ad88889daee02904f82aa07ae91a4c69e6ec58|3        |\n",
      "|27bdc71bcc167a89e07e981246320dbdaddc8c67354ed022f3e835368da0fb0d|3        |\n",
      "|ca5fd93b4f9adf94118962b8d3e3d24e4810d891d1dbcaa0f98e641e16d4b4a2|3        |\n",
      "|a6ae6eb048d08c8f35eb7fd7f7820b5f39af5c6800a59c68790275d696f1dd7c|2        |\n",
      "+----------------------------------------------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.loan_defaulters_detail_rec GROUP BY member_id  ORDER BY total_cnt DESC;\n",
    "          \"\"\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f61a15c",
   "metadata": {},
   "source": [
    "###### we can observe count of member_id in the table are multiple which implies these each id is having different data then which data should we consider ? This should be decided by upstream team for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef95bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_data = spark.sql(\"SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.customers GROUP BY member_id HAVING total_cnt > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a11d597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeating_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8f2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_customer_data = spark.sql(\"\"\"SELECT member_id FROM (SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.customers\n",
    "                            GROUP BY member_id HAVING total_cnt > 1)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84d687be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeating_customer_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc6a0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_loan_defaulters_df= spark.sql(\"\"\"SELECT member_id FROM (SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.delinq\n",
    "                            GROUP BY member_id HAVING total_cnt > 1)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4241a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeating_loan_defaulters_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d50b710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_loan_defaulters_detail_df= spark.sql(\"\"\"SELECT member_id FROM (SELECT member_id,count(*) AS total_cnt FROM lendingclubdata.loan_defaulters_detail_rec\n",
    "                            GROUP BY member_id HAVING total_cnt > 1)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37438abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeating_loan_defaulters_detail_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "116b0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_customer_data.repartition(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").option(\"path\",\"Lending_club_project/bad_data/bad_customers_data_csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3f32a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_loan_defaulters_df.repartition(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").option(\"path\",\"Lending_club_project/bad_data/bad_loan_defaulters_data_csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88b868a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_loan_defaulters_detail_df.repartition(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").option(\"path\",\"Lending_club_project/bad_data/bad_loan_defaulters_detailed_data_csv\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f45d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cust_data = repeating_customer_data.select(\"member_id\") \\\n",
    ".union(repeating_loan_defaulters_detail_df.select(\"member_id\")) \\\n",
    ".union(repeating_loan_defaulters_df.select(\"member_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d522a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bad_cust_df = bad_cust_data.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab866add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bad_cust_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f584e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bad_cust_df.repartition(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").option(\"path\",\"Lending_club_project/bad_data/final_bad_customer_data_csv\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd5fa7",
   "metadata": {},
   "source": [
    "##### Now we can ignore these member_id for processing further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5ed93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to remove these data from our data\n",
    "final_bad_cust_df.createOrReplaceTempView(\"bad_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "807dfce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------+-----------+\n",
      "|namespace      |tableName                 |isTemporary|\n",
      "+---------------+--------------------------+-----------+\n",
      "|lendingclubdata|customer_complete_view    |false      |\n",
      "|lendingclubdata|customers                 |false      |\n",
      "|lendingclubdata|delinq                    |false      |\n",
      "|lendingclubdata|loan_defaulters_detail_rec|false      |\n",
      "|lendingclubdata|loan_repayments           |false      |\n",
      "|lendingclubdata|loans                     |false      |\n",
      "|               |bad_data                  |true       |\n",
      "+---------------+--------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables from lendingclubdata\").show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f55835",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_final_df =spark.sql(\"select * from lendingclubdata.customers where member_id NOT IN (SELECT member_id from bad_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fba92fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(member_id)|\n",
      "+----------------+\n",
      "|          717316|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(member_id) from lendingclubdata.customers where member_id NOT IN (SELECT member_id from bad_data)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6be29406",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_final_df.write.format(\"parquet\").mode(\"overwrite\").option(\"path\",\"Lending_club_project/cleaned_new/customer_parquet\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a33efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE database lendingclubdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0255960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loan_defaulters_final_df =spark.sql(\"select * from lendingclubdata.delinq where member_id NOT IN (SELECT member_id from bad_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8c784e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363103"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_defaulters_final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6b7aa65",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o104.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2524)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:153)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:152)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:525)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:486)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:482)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:482)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:233)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:367)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:352)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:213)\n\t... 41 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20/785753516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloan_defaulters_final_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Lending_club_project/cleaned_new/loan_defaulters_parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o104.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:829)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2524)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:153)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:152)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:525)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:486)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:482)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:482)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:516)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:516)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:233)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:367)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:352)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:213)\n\t... 41 more\n"
     ]
    }
   ],
   "source": [
    "loan_defaulters_final_df.write.format(\"parquet\").mode(\"overwrite\").option(\"path\",\"Lending_club_project/cleaned_new/loan_defaulters_parquet\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_defaulters_detailed_final_df =spark.sql(\"select * from lendingclubdata.loan_defaulters_detail_rec where member_id NOT IN (SELECT member_id from bad_data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9cc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------+\n",
      "|      namespace|           tableName|isTemporary|\n",
      "+---------------+--------------------+-----------+\n",
      "|lendingclubdata|customer_complete...|      false|\n",
      "|lendingclubdata|           customers|      false|\n",
      "|lendingclubdata|              delinq|      false|\n",
      "|lendingclubdata|loan_defaulters_d...|      false|\n",
      "|lendingclubdata|     loan_repayments|      false|\n",
      "|lendingclubdata|               loans|      false|\n",
      "+---------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"show tables from lendingclubdata \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_defaulters_detailed_final_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
